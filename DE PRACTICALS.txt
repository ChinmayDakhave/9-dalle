												DE PRACTICALS 

PRACTICAL 1 - 
A - 
1....Parallelize, 
2....Read text file, 
3....Read CSV, 
4....Create RDD, 
5....Actions, 
6....Pair Functions, 
7....Repartition and Coalesce, 
8....Shuffle Partitions, 
9....Broadcast Variables, 
10....Accumulator Variables 
11....Convert RDD to DataFrame
											LOADING COMMANDS (A MUST TO RUN)


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructField, StructType, StringType, IntegerType}

val spark = SparkSession.builder() .appName("Spark Operations Example").master("local[*]").getOrCreate()



												1.Parallelize

val data = Seq(1, 2, 3, 4, 5)
val rdd = spark.sparkContext.parallelize(data)

												2.Read Text File


val textFileRDD = spark.sparkContext.textFile("C:/Users/Administrator/Desktop/textfile.txt")

textFileRDD.take(5).foreach(println)

												3.Read CSV



val csvDF = spark.read.option("header", "true").csv("C:/Users/Administrator/Desktop/10.csv")

csvDF.show() 

												4.Create RDD



val pairRDD = spark.sparkContext.parallelize(Seq(("apple", 1), ("banana", 2), ("orange", 3)))


												5.Actions


val count = pairRDD.count()
println(s"Count of pairs: $count")
       val collectedData = pairRDD.collect()
collectedData.foreach(println) 

												6.Pair Functions


val reducedRDD = pairRDD.reduceByKey((x, y) => x + y)

reducedRDD.collect().foreach(println) 

												7.Repartition and Coalesce


val repartitionedRDD = pairRDD.repartition(3)
println(s"Repartitioned RDD has ${repartitionedRDD.getNumPartitions} partitions")
val coalescedRDD = repartitionedRDD.coalesce(2)
println(s"Coalesced RDD has ${coalescedRDD.getNumPartitions} partitions")

												8.Shuffle Partitions


spark.conf.set("spark.sql.shuffle.partitions", "4")


												9.Broadcast Variables


val broadcastVar = spark.sparkContext.broadcast(Array(1, 2, 3))
val broadcastedRDD = rdd.map(x => (x, broadcastVar.value.contains(x)))
broadcastedRDD.collect().foreach(println)

												10.Accumulator Variables


val accum = spark.sparkContext.longAccumulator("Sum Accumulator")
rdd.foreach(x => accum.add(x))
println(s"Accumulated sum: ${accum.value}")

												11.Convert RDD to DataFrame


val schema = StructType(Array(
  StructField("Fruit", StringType, true),
  StructField("Quantity", IntegerType, true)
))
val fruitRDD = spark.sparkContext.parallelize(Seq(("apple", 10), ("banana", 20), ("orange", 30)))
val fruitDF = spark.createDataFrame(fruitRDD.map(Row.fromTuple), schema)
fruitDF.show()




												PRACTICAL 1 - B 
1....Read multiple text files into RDD, 
2....Read CSV fileinto RDD, 
3....Create an empty RDD, 
4....RDD Pair Functions 
5....Generate DataFrame from RDD


												LOADING COMMAND 

import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD

val spark = SparkSession.builder().appName("RDD Operations Example").master("local[*]").getOrCreate()


											1.Read multiple text files into RDD


val textFilesRDD: RDD[String] = spark.sparkContext.textFile("C:/depract1/text1.txt, C:/depract1/text2.txt, C:/depract1/text3.txt")

println("Contents of the text files RDD:")

textFilesRDD.collect().foreach(println)


											2.Read CSV file into RDD



val csvFileRDD: RDD[String] = spark.sparkContext.textFile("C:/Users/Administrator/Desktop/example.csv")

println("Contents of the CSV file RDD:")

csvFileRDD.collect().foreach(println)


											3. Create an empty RDD 



val emptyRDD: RDD[String] = spark.sparkContext.emptyRDD[String]

println("Number of elements in the empty RDD: " + emptyRDD.count())

											4. RDD Pair Functions



val pairRDD = sc.parallelize(List(("apple", 2), ("orange", 1), ("apple", 3), ("orange", 4)))
val reducedRDD = pairRDD.reduceByKey(_ + _)

reducedRDD.collect().foreach(println)
val groupedRDD = pairRDD.groupByKey()

groupedRDD.collect().foreach{ case (key, value) => println(s"$key -> ${value.mkString(", ")}")}


											5. Generate DataFrame from RDD



import spark.implicits._

val dataRDD = sc.parallelize(Seq(("John", 30), ("Jane", 25), ("Sam", 20)))

val df = dataRDD.toDF("Name", "Age")

df.show()





									PRACTICAL 2 - Practical on the DataFrame operations 


									A: Demonstrate the use of next mentioned operations: 
1...Create an empty  DataFrame,
2...Create an empty DataSet,
3...use of Rename nested column, 
4...Adding  or Updating a column on DataFrame, 
5...Drop a column on DataFrame, 
6...Adding  literal constant to DataFrame, 
7...Changing column data type, 
8...Pivot and Unpivot  a DataFrame, 
9...Create a DataFrame using StructType & StructField schema




										1:Create an Empty DataFrame


import org.apache.spark.sql.{SparkSession, DataFrame}

val schema = StructType(Array(StructField("column1",
 StringType, nullable = true),
  StructField("column2", IntegerType, nullable = true)))
 
val emptyRDD = spark.sparkContext.emptyRDD[Row]
val emptyDF = spark.createDataFrame(emptyRDD, schema)

emptyDF.show()



										2: Create an Empty DataSet



import org.apache.spark.sql.{Dataset, Encoders}

case class Person(id: Int, name: String)

val emptyDS: Dataset[Person] = spark.emptyDataset[Person]


emptyDS.show()



										3: Rename Nested Column


import org.apache.spark.sql.functions._


val df = Seq(
  (1, ("John", "Doe")),
  (2, ("Jane", "Smith"))
).toDF("id", "name_tuple")


val dfRenamed = df
  .withColumn("first_name", col("name_tuple._1"))
  .withColumn("last_name", col("name_tuple._2"))
  .drop("name_tuple")


dfRenamed.show()


										4:Adding or Updating a Column on DataFrame


val df = Seq((1, "John"), (2, "Jane")).toDF("id", "name")


val dfUpdated = df.withColumn("age", lit(30))


dfUpdated.show()


										5:Drop a Column on DataFrame



val df = Seq((1, "John", 30), (2, "Jane", 25)).toDF("id", "name", "age")


val dfDropped = df.drop("age")


dfDropped.show()



										6:Adding a Literal Constant to DataFrame



val df = Seq((1, "John"), (2, "Jane")).toDF("id", "name")


val dfWithConstant = df.withColumn("status", lit("active"))


dfWithConstant.show()



										7. Changing Column Data Type


val df = Seq(("1", "John"), ("2", "Jane")).toDF("id", "name")


val dfWithCast = df.withColumn("id", col("id").cast(IntegerType))


dfWithCast.show()



										8. Pivot and Unpivot a DataFrame



val df = Seq(
  ("2024-01-01", "A", 10),
  ("2024-01-01", "B", 20),
  ("2024-01-02", "A", 30),
  ("2024-01-02", "B", 40)
).toDF("date", "category", "value")


val dfPivot = df.groupBy("date").pivot("category").sum("value")
dfPivot.show()


val dfUnpivot = dfPivot.selectExpr("date", "A as category_A", "B as category_B")
dfUnpivot.show()



										9. Create a DataFrame Using StructType & StructField Schema




val schema = StructType(Array(
  StructField("id", IntegerType, nullable = true),
  StructField("name", StringType, nullable = true),
  StructField("age", IntegerType, nullable = true)
))


val data = Seq(
  Row(1, "John", 30),
  Row(2, "Jane", 25)
)
val rdd = spark.sparkContext.parallelize(data)
val dfWithSchema = spark.createDataFrame(rdd, schema)

dfWithSchema.show()



										2 B:Use of next mentioned operations: 

1....Selecting the first row of each group, 
2....Sort  DataFrame, 
3....Union DataFrame, 
4....Drop Rows with null values from DataFrame, 
5....Split  single to multiple columns,
6....Concatenate multiple columns, 
7....Replace null values in  DataFrame, 
8....Remove duplicate rows on DataFrame, 
9....Remove distinct on multiple  selected columns, 
10....Spark UDF practical example


										1. Selecting the First Row of Each Group


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder.appName("example").getOrCreate()

import spark.implicits._


val df = Seq(("A", 1), ("A", 2), ("B", 3), ("B", 4)).toDF("group", "value")

val dfFirst = df.groupBy("group").agg(first("value").alias("first_value"))

dfFirst.show()


										2. Sort DataFrame



val df = Seq((1, "B"), (2, "A"), (3, "C")).toDF("id", "letter")

val dfSorted = df.orderBy("letter")

dfSorted.show()



										3. Union DataFrame


val df1 = Seq((1, "A"), (2, "B")).toDF("id", "letter")
val df2 = Seq((3, "C"), (4, "D")).toDF("id", "letter")

val dfUnion = df1.union(df2)

dfUnion.show()



									4. Drop Rows with Null Values from DataFrame



val df = Seq((1, "A"), (2, null.asInstanceOf[String]), (null.asInstanceOf[Int], "B")).toDF("id", "letter")

val dfNoNulls = df.na.drop()

dfNoNulls.show()



									5. Split Single Column into Multiple Columns



import org.apache.spark.sql.functions._

val df = Seq(("John_Doe"), ("Jane_Smith")).toDF("full_name")

val dfSplit = df.withColumn("first_name", split(col("full_name"), "_").getItem(0))

dfSplit.show()




									6. Concatenate Multiple Columns



val df = Seq(("John", "Doe"), ("Jane", "Smith")).toDF("first_name", "last_name")

val dfConcatenated = df.withColumn("full_name", concat(col("first_name"), lit(" "), col("last_name")))

dfConcatenated.show()




									7. Replace Null Values in DataFrame



val df = Seq((1, "A"), (null.asInstanceOf[Int], "B"), (3,null.asInstanceOf[String])).toDF("id", "letter")

val dfFilled = df.na.fill(Map("id" -> 0, "letter" -> "Unknown"))

dfFilled.show()




									8. Remove Duplicate Rows from DataFrame


val df = Seq((1, "A"), (1, "A"), (2, "B")).toDF("id", "letter")

val dfNoDuplicates = df.dropDuplicates()

dfNoDuplicates.show()




									9. Remove Distinct on Multiple Selected Columns



val df = Seq((1, "A", 10), (1, "A", 20), (2, "B", 30)).toDF("id", "letter","value")

val dfNoDuplicatesSubset = df.dropDuplicates(Seq("id", "letter"))

dfNoDuplicatesSubset.show()




									10. Spark UDF Practical Example




import org.apache.spark.sql.functions._


val addOneUDF = udf((x: Int) => x + 1)


val df = Seq((1), (2), (3)).toDF("number")


val dfWithUDF = df.withColumn("number_plus_one", addOneUDF(col("number")))

dfWithUDF.show()




												PRACTICAL 3 

                    										(A)
1....Create an Array (ArrayType) Column on DataFrame
2....Create a Map (MapType) Column on DataFrame
3....Convert an Array to Columns
4....Create an Array of Struct Column
5....Explode an Array and Map Columns
6....Explode an Array of Structs
7....Explode an Array of Map Columns to Rows

									
		
										1.Create an Array (ArrayType) Column on DataFrame


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

val spark = SparkSession.builder().appName("ArrayType Column Example") .getOrCreate()

import spark.implicits._

val dfArray = Seq( (1, Array("a", "b", "c")), (2, Array("d", "e")), (3, Array("f", "g", "h", "i")) ).toDF("id", "letters")

dfArray.show()


										2.Create a Map (MapType) Column on DataFrame



val dfMap = Seq( (1, Map("a" -> 1, "b" -> 2)),(2, Map("c" -> 3, "d" -> 4))).toDF("id", "value_map")

dfMap.show()


										3.Convert an Array to Columns



val dfConverted = dfArray.withColumn("first", $"letters".getItem(0))  .withColumn("second", $"letters".getItem(1))

dfConverted.show()


										4.Create an Array of Struct Column



val dfStruct = Seq( (1, Array(("a", 1), ("b", 2))), (2, Array(("c", 3), ("d", 4)))).toDF("id", "struct_array")

dfStruct.show()


										5.Explode an Array and Map Columns


val explodedDf = dfArray.withColumn("letter", explode($"letters")) 

explodedDf.show()


										6.Explode an Array of Structs



val explodedStructDf = dfStruct.withColumn("struct", explode($"struct_array"))

explodedStructDf.show()


										7.Explode an Array of Map Columns to Rows



val explodedMapDf = dfMap.select($"*", explode($"value_map").as(Seq("key", "value")))

explodedMapDf.show()


                    									 Practical No.3(B)

1...Create a DataFrame with Nested Arrays
2...Explode Nested Arrays to Rows
3...Flatten Nested Array to Single Array
4...Convert Array of Strings to a Single String Column


										1.Create a DataFrame with Nested Arrays

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder().appName("Nested Array Example").getOrCreate()

import spark.implicits._

val nestedArrayDf = Seq((1, Array(Array("a", "b"), Array("c", "d"))),(2, Array(Array("e", "f"))),  (3, Array(Array("g", "h"), Array("i", "j", "k")))).toDF("id", "nested_array")

nestedArrayDf.show(truncate = false)


										2.Explode Nested Arrays to Rows




val explodedNestedDf = nestedArrayDf.withColumn("exploded", explode($"nested_array")).select($"id", $"exploded")

explodedNestedDf.show(truncate = false)

							
										3.Flatten Nested Array to Single Array



val flattenedDf = nestedArrayDf.select($"id", explode_outer($"nested_array").as("inner_array")).select($"id", $"inner_array") .groupBy("id")  .agg(collect_list($"inner_array").as("flattened_array"))

flattenedDf.show(truncate = false)


										4.Convert Array of Strings to a Single String Column


val stringArrayDf = Seq(
  (1, Array("Hello", "World")),
  (2, Array("Apache", "Spark")),
  (3, Array("Scala", "Rocks"))
).toDF("id", "string_array")

val stringColumnDf = stringArrayDf.withColumn("combined_string", concat_ws(" ", $"string_array"))

stringColumnDf.show(truncate = false)




                     											Practical NO.4

1....Group Rows in DataFrame
2....Get Count Distinct on DataFrame
3....Add Row Number to DataFrame
4....Select the First Row of Each Group


												1.Group Rows in DataFrame


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder().appName("Spark Aggregate Example").master("local[*]").getOrCreate()

import spark.implicits._	

val df = Seq(
  (“Alice”, “2024-01-01”),
  (“Bob”, “2024-01-02”),
  (“Alice”, “2024-01-03”),
  (“Bob”, “2024-01-04”),
  (“Cathy”, “2024-01-05”)
).toDF(“name”, “date”)

val groupedDf = df.groupBy("name").count()
groupedDf.show()

												2.Get Count Distinct on DataFrame



val distinctCountDf = df.select(countDistinct("name").alias("distinct_names_count"))

distinctCountDf.show()

												3.Add Row Number to DataFrame




import org.apache.spark.sql.expressions.Window

val windowSpec = Window.orderBy("name")

val dfWithRowNum = df.withColumn("row_number", row_number().over(windowSpec))

dfWithRowNum.show()

												4.Select the First Row of Each Group



import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

val windowSpec = Window.partitionBy("name").orderBy("date")

val dfWithRowNum = df.withColumn("row_number", row_number().over(windowSpec))

val firstRowDF = dfWithRowNum.filter($"row_number" === 1).select("name", "date")

firstRowDF.show()




										PRACTICAL 5 - Spark SQL Joins, Spark SQL Schema, StructType &amp; SQL Functions


													5 A 
1....Use of Spark SQL Join, 
2....Join multiple
3....DataFrames, 
4....Inner join two tables/DataFrame, 
5....Self join, 
6....Join tables on multiple columns,
7....Convert case class to a schema, 
8....Create array of struct column, 
9....Flatten nested column 


											1. Use of Spark SQL Join

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder().appName("Spark SQL Join Example").master("local[*]").getOrCreate()

import spark.implicits._

val df1 = Seq((1, "Alice"),(2, "Bob"),(3, "Cathy")).toDF("id", "name")

val df2 = Seq((1, "New York"),(2, "Los Angeles"),(4, "Chicago")).toDF("id", "city")


											2. Inner Join Two DataFrames

	
val innerJoinDf = df1.join(df2, Seq("id"), "inner")

innerJoinDf.show()


											3. Self Join


val selfJoinDf = df1.as("a").join(df1.as("b"), $"a.id" =!= $"b.id").select($"a.name".alias("NameA"), $"b.name".alias("NameB"))

selfJoinDf.show()


											4.Join Tables on Multiple Columns


val df3 = Seq( (1, "New York", 25),(2, "Los Angeles", 30), (1, "San Francisco", 40)).toDF("id", "city", "age")

val joinMultipleDf = df1.join(df3, Seq("id"), "inner")

joinMultipleDf.show()

											5.Convert Case Class to a Schema


import org.apache.spark.sql.SparkSession

case class Person(id: Int, name: String, age: Int)

val spark = SparkSession.builder().appName("Example")  .master("local").getOrCreate()

val caseClassDf = spark.createDataFrame(Seq(
  Person(1, "Alice", 25),
  Person(2, "Bob", 30),
  Person(3, "Cathy", 35)
))

caseClassDf.show()

											6. Create Array of Struct Column

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val arrayOfStructDf = Seq(
  (1, Seq((1, "Math"), (2, "Science"))),
  (2, Seq((3, "English"), (4, "History")))
).toDF("id", "subjects")

val structDf = arrayOfStructDf.withColumn(
  "subjects",
  $"subjects".cast(
    ArrayType(
      StructType(Seq(
        StructField("subject_id", IntegerType),
        StructField("subject_name", StringType)
      ))
    )
  )
)

structDf.show(false)

											7.Flatten Nested Column


val flattenedDf = structDf.select($"id", explode($"subjects").as("subject")).select($"id", $"subject.subject_id", $"subject.subject_name")

flattenedDf.show()



											      5  B 
1....Date and Time Functions
2....String Functions
3....Array Functions
4....Map Functions
5....Aggregate Functions
6....Window Functions
7....Sort Functions
8....JSON Functions

 

											1.Date and Time Functions

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder().appName("Date Manipulation Example").master("local").getOrCreate()

val df = Seq(
  ("2024-09-24"),
  ("2024-09-25"),
  ("2024-09-26")
).toDF("date_str")

val dateDf = df.withColumn("date", to_date($"date_str"))

val updatedDf = dateDf .withColumn("next_day", next_day($"date", "SUNDAY")).withColumn("date_plus_5", date_add($"date", 5))

updatedDf.show()

											2.String Functions


val stringDf = Seq(("Alice"),("Bob"),("Cathy")).toDF("name")

val stringWithFunctions = stringDf.withColumn("upper_name", upper($"name")).withColumn("name_length", length($"name"))

stringWithFunctions.show()

											3.Array Functions


val arrayDf = Seq((1, Array("apple", "banana")), (2, Array("orange", "grape")),(3, Array("kiwi", "mango"))).toDF("id", "fruits")

val arrayWithFunctions = arrayDf.withColumn("fruit_count", size($"fruits")).withColumn("first_fruit", element_at($"fruits", 1))

arrayWithFunctions.show()

											4.Map Functions


val mapDf = Seq(
  (1, Map("A" -> 1, "B" -> 2)),
  (2, Map("C" -> 3, "D" -> 4))
).toDF("id", "letters")

val mapWithFunctions = mapDf.withColumn("keys", map_keys($"letters")).withColumn("values", map_values($"letters"))

mapWithFunctions.show(false)

											5.Aggregate Functions

val aggDf = Seq(
  ("Alice", 10),
  ("Bob", 20),
  ("Alice", 30),
  ("Bob", 40)
).toDF("name", "score")

val aggWithFunctions = aggDf
  .groupBy("name")
  .agg(
    sum("score").alias("total_score"),
    avg("score").alias("average_score")
  )

aggWithFunctions.show()

											6.Window Functions

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window


val spark = SparkSession.builder()
  .appName("Window Function Example")
  .master("local[*]") 
  .getOrCreate()


val aggDf = Seq(
  ("Alice", 10),
  ("Bob", 20),
  ("Alice", 30),
  ("Bob", 40)
).toDF("name", "score")

val windowSpec = Window.partitionBy("name").orderBy("score")

aggDf.withColumn("rank", rank().over(windowSpec)).show()

											7.Sort Functions

val sortedDf = aggDf.sort($"score".desc)

sortedDf.show()

											8. JSON Functions

val jsonDf = Seq(
  """{"name":"Alice","age":25}""",
  """{"name":"Bob","age":30}"""
).toDF("json_string")
val jsonWithFunctions = jsonDf
  .select(from_json($"json_string", schema = "name STRING, age INT").alias("data"))
  .select($"data.*")
jsonWithFunctions.show()



												PRACTICAL 6 

1....createDataFrame()
2....where() & filter()
3....withColumn()
4....withColumnRenamed()
5....drop()
6....distinct()
7....groupBy()
8....join()
9....map() vs mapPartitions()
10...foreach() vs foreachPartition()
11...pivot()
12...union()
13...collect()
14...cache() & persist()
15...udf()

											1.createDataFrame()
									you are on your own , no working code found for this 




											2. where() & filter()



import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import spark.implicits._

val spark = SparkSession.builder()
  .appName("FilterExample")
  .master("local[*]")
  .getOrCreate()

val data = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns = Seq("Name", "Age")

val df = data.toDF(columns: _*)   
val filteredDF = df.where("Age > 25").filter("Name != 'Jane'")

filteredDF.show()


											3. withColumn()



import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("WithColumnExample").master("local[*]").
getOrCreate()

// Creating DataFrame
val data = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns = Seq("Name", "Age")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using withColumn()
val doubledAgeDF = df.withColumn("DoubleAge", col("Age") * 2)
doubledAgeDF.show()


											4.withColumnRenamed()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("WithColumnRenamedExample").master("loca
l[*]").getOrCreate()
// Creating DataFrame
val data = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns = Seq("Name", "Age")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using withColumnRenamed()
val renamedDF = df.withColumnRenamed("Age", "Years")
renamedDF.show()


											5.drop()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("DropExample").master("local[*]").getOrC
reate()
// Creating DataFrame
val data = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns = Seq("Name", "Age")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using drop()
val withoutAgeDF = df.drop("Age")
withoutAgeDF.show()

											6.distinct()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("DistinctExample").master("local[*]").ge
tOrCreate()

// Creating DataFrame
val data = Seq(("John", 28), ("Jane", 22), ("John", 28), ("Doe", 35))
val columns = Seq("Name", "Age")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using distinct()
val distinctDF = df.distinct()
distinctDF.show()

											7. groupBy()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("GroupByExample").master("local[*]").get
OrCreate()
// Creating DataFrame
val data = Seq(("John", "Math", 90), ("Jane", "Math", 85), ("Doe",
"History", 75))
val columns = Seq("Name", "Subject", "Score")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using groupBy()
val avgScoreDF = df.groupBy("Subject").agg(avg("Score").as("AvgScore"))
avgScoreDF.show()


											8. join()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("JoinExample").master("local[*]").getOrC
reate()
// Creating DataFrames
val data1 = Seq(("John", "Math", 90), ("Jane", "Math", 85), ("Doe",
"History", 75))
val columns1 = Seq("Name", "Subject", "Score")
val df1 = spark.createDataFrame(data1).toDF(columns1: _*)
val data2 = Seq(("John", "USA"), ("Jane", "Canada"), ("Doe", "USA"))
val columns2 = Seq("Name", "Country")
val df2 = spark.createDataFrame(data2).toDF(columns2: _*)
// Using join()
val resultDF = df1.join(df2, "Name")
resultDF.show()


										9. map() vs mapPartitions()



import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("MapMapPartitionsExample").master("local
[*]").getOrCreate()
// Creating DataFrame
val data = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns = Seq("Name", "Age")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using map()
val mappedDF = df.map(row => (row.getString(0), row.getInt(1) * 2))
// Using mapPartitions()
val mappedPartitionsDF = df.mapPartitions(rows => rows.map(row =>
(row.getString(0), row.getInt(1) * 2)))
mappedDF.show()
mappedPartitionsDF.show()

										10.  foreach() vs foreachPartition()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("ForeachForeachPartitionExample").master
("local[*]").getOrCreate()
// Creating DataFrame
val data = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns = Seq("Name", "Age")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using foreach()
df.foreach(row => println(row.getString(0)))
// Using foreachPartition()
df.foreachPartition(rows => rows.foreach(row =>println(row.getString(0))))
spark.stop()


											11. pivot()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._

val spark=SparkSession.builder().appName("PivotExample").master("local[*]").getOrCreate()
val data = Seq(("John", "Math", 90), ("Jane", "Math", 85), ("Doe","History", 75))
val columns = Seq("Name", "Subject", "Score")
val df = spark.createDataFrame(data).toDF(columns: _*)
val pivotedDF = df.groupBy("Name").pivot("Subject").agg(first("Score"))
pivotedDF.show()

											12. union()



import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("UnionExample").master("local[*]").getOr
Create()
// Creating DataFrames
val data1 = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns1 = Seq("Name", "Age")
val df1 = spark.createDataFrame(data1).toDF(columns1: _*)
val data2 = Seq(("Alice", 25), ("Bob", 30))
val columns2 = Seq("Name", "Age")
val df2 = spark.createDataFrame(data2).toDF(columns2: _*)
// Using union()
val unionDF = df1.union(df2)
unionDF.show()



											13. collect()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("CollectExample").master("local[*]").get
OrCreate()
// Creating DataFrame
val data = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns = Seq("Name", "Age")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using collect()
val collectedArray = df.collect()
collectedArray.foreach(println)


										14.cache() & persist()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
val spark =
SparkSession.builder().appName("CachePersistExample").master("local[*]"
).getOrCreate()
// Creating DataFrame
val data = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns = Seq("Name", "Age")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using cache() & persist()
val cachedDF = df.cache()
cachedDF.show()


											15. udf()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.UserDefinedFunction
val spark =
SparkSession.builder().appName("UDFExample").master("local[*]").getOrCr
eate()
// Creating DataFrame
val data = Seq(("John", 28), ("Jane", 22), ("Doe", 35))
val columns = Seq("Name", "Age")
val df = spark.createDataFrame(data).toDF(columns: _*)
// Using udf()
val squareUDF: UserDefinedFunction = udf((value: Int) => value * value)
val squaredDF = df.withColumn("SquaredAge", squareUDF(col("Age")))
squaredDF.show()


